{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e001c8bf-1ff3-403b-8077-25246059bdfb",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd84ec2-3081-4073-a771-cf6bbc5e3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/emo-rec/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import openai\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import neattext.functions as nfx\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8db58d-2464-4f03-871a-4559047906dc",
   "metadata": {},
   "source": [
    "# Load MELD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf98448-6d52-4eb6-9c2a-117358627687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"datasets/meld/train_sent_emo.csv\")\n",
    "df_test = pd.read_csv(\"datasets/meld/test_sent_emo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76e9bd5-d90d-4e5f-87a6-c360588c53f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'surprise', 'fear', 'sadness', 'joy', 'disgust', 'anger']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"I thought things would get better with time, but it just keeps hurting more.\"\n",
    "emotion_labels = df_train[\"Emotion\"].unique().tolist()\n",
    "emotion_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5396df-7945-4809-85f3-f20875a99618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9989 entries, 0 to 9988\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Sr No.        9989 non-null   int64 \n",
      " 1   Utterance     9989 non-null   object\n",
      " 2   Speaker       9989 non-null   object\n",
      " 3   Emotion       9989 non-null   object\n",
      " 4   Sentiment     9989 non-null   object\n",
      " 5   Dialogue_ID   9989 non-null   int64 \n",
      " 6   Utterance_ID  9989 non-null   int64 \n",
      " 7   Season        9989 non-null   int64 \n",
      " 8   Episode       9989 non-null   int64 \n",
      " 9   StartTime     9989 non-null   object\n",
      " 10  EndTime       9989 non-null   object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 858.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb39103-3927-4c2d-9b2b-df48f5598e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance          Speaker  \\\n",
       "0       1  also I was the point person on my company’s tr...         Chandler   \n",
       "1       2                   You must’ve had your hands full.  The Interviewer   \n",
       "2       3                            That I did. That I did.         Chandler   \n",
       "3       4      So let’s talk a little bit about your duties.  The Interviewer   \n",
       "4       5                             My duties?  All right.         Chandler   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            0             0       8       21   \n",
       "1   neutral   neutral            0             1       8       21   \n",
       "2   neutral   neutral            0             2       8       21   \n",
       "3   neutral   neutral            0             3       8       21   \n",
       "4  surprise  positive            0             4       8       21   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:16:16,059  00:16:21,731  \n",
       "1  00:16:21,940  00:16:23,442  \n",
       "2  00:16:23,442  00:16:26,389  \n",
       "3  00:16:26,820  00:16:29,572  \n",
       "4  00:16:34,452  00:16:40,917  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ecf32b-b2c8-41da-91c5-c2e8cc6a4ac9",
   "metadata": {},
   "source": [
    "### 📊 Emotion Distribution in the MELD Training Set\n",
    "\n",
    "Before training any models, I explore the class distribution of emotions in the MELD training dataset. \n",
    "    \n",
    "To visualize this, I create a **pie chart** showing the relative frequency of each emotion label, with percentage values displayed in the legend. This provides an intuitive overview of the emotional makeup of the dataset and helps inform decisions regarding model evaluation and potential rebalancing strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1646c827-c13d-45fc-a76d-a486d51a3c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral     4710\n",
       "joy         1743\n",
       "surprise    1205\n",
       "anger       1109\n",
       "sadness      683\n",
       "disgust      271\n",
       "fear         268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"Emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662c7f99-439c-4a33-a8d3-91d45a034f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hole": 0.3,
         "labels": [
          "neutral (47.15%)",
          "joy (17.45%)",
          "surprise (12.06%)",
          "anger (11.10%)",
          "sadness (6.84%)",
          "disgust (2.71%)",
          "fear (2.68%)"
         ],
         "textinfo": "none",
         "type": "pie",
         "values": [
          4710,
          1743,
          1205,
          1109,
          683,
          271,
          268
         ]
        }
       ],
       "layout": {
        "height": 450,
        "legend": {
         "font": {
          "size": 12
         },
         "orientation": "v",
         "x": 1.02,
         "xanchor": "left",
         "y": 1,
         "yanchor": "top"
        },
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 50
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Emotion Distribution in MELD Dataset",
         "x": 0.5
        },
        "width": 600
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get counts and percentages\n",
    "df_value_counts = df_train[\"Emotion\"].value_counts()\n",
    "labels = list(df_value_counts.keys())\n",
    "sizes = df_value_counts.tolist()\n",
    "\n",
    "# Calculate formatted labels: \"emotion (xx.xx%)\"\n",
    "total = sum(sizes)\n",
    "legend_labels = [f\"{label} ({(count/total)*100:.2f}%)\" for label, count in zip(labels, sizes)]\n",
    "\n",
    "# Create pie chart\n",
    "fig = go.Figure(\n",
    "    data=[go.Pie(\n",
    "        labels=legend_labels,     # legend-style labels\n",
    "        values=sizes,\n",
    "        textinfo='none',          # remove text from inside the pie\n",
    "        hole=0.3\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Update layout: legend in top-right\n",
    "fig.update_layout(\n",
    "    title=\"Emotion Distribution in MELD Dataset\",\n",
    "    width=600,\n",
    "    height=450,\n",
    "    margin=dict(l=20, r=20, t=50, b=20),\n",
    "    title_x=0.5,\n",
    "    legend=dict(\n",
    "        orientation=\"v\",\n",
    "        x=1.02,\n",
    "        y=1,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "        font=dict(size=12)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9365a11-95b1-4da7-8302-607e172a6915",
   "metadata": {},
   "source": [
    "**To ensure faster evaluation and reduce API costs during large language model inference, I limit the MELD test set to the first 500 examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04c840e-bd38-4f48-a8ca-830877183c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size is reduced from 2610 to 500\n"
     ]
    }
   ],
   "source": [
    "test_size = 500\n",
    "print(f\"test size is reduced from {len(df_test)} to {test_size}\")\n",
    "df_test = df_test[:test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae6963-3177-4c21-b45b-15fc5803703f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b9ef6f-c821-4cd6-9051-915faa0dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {} # save weighted f1 scores for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39f63a-1714-4a7a-a989-99732591977d",
   "metadata": {},
   "source": [
    "## 🧪 Supervised Emotion Classification with Traditional ML Models\n",
    "\n",
    "In this section, I apply three traditional supervised learning models—**Logistic Regression**, **Support Vector Classifier (SVC)**, and **Random Forest Classifier**—to the MELD dataset for text-based emotion classification.\n",
    "\n",
    "First, I preprocess the training utterances by removing user handles and stopwords. Then, I use the `CountVectorizer` to convert the cleaned text into numerical feature vectors. These vectors serve as input to the classifiers.\n",
    "\n",
    "Each model is wrapped in a `scikit-learn` pipeline for streamlined preprocessing and training. After training, I evaluate the models on the test split of the MELD dataset using standard classification metrics (precision, recall, and F1-score).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c0cf2-8fc2-4a9c-baa8-32dc20b880e7",
   "metadata": {},
   "source": [
    "Preprocess the utterances and assign the cleaned utterances and corresponding emotions to x_train and y_train, respectively, for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf5b99c-0d99-4e0e-bc4c-43dc78043647",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = df_train[\"Utterance\"].apply(nfx.remove_userhandles).apply(nfx.remove_stopwords), df_train['Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25690a9-1e12-4eb1-a522-7e98c6129b68",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ab49d6-1474-4e55-ba53-c45c39138cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regrestion model is ready to use!\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression(max_iter=100))])\n",
    "pipe_lr.fit(x_train, y_train)\n",
    "print(\"Logistic regrestion model is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79bca585-52da-4140-9a44-79120a60e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = pipe_lr.predict(df_test[\"Utterance\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f25dfa-c09b-462c-9f31-24506436246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on the MELD dataset using Logistic Regression:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.35      0.26      0.30        62\n",
      "     disgust       0.33      0.09      0.14        11\n",
      "        fear       0.00      0.00      0.00        10\n",
      "         joy       0.28      0.18      0.22        91\n",
      "     neutral       0.54      0.64      0.59       238\n",
      "     sadness       0.31      0.32      0.31        38\n",
      "    surprise       0.27      0.38      0.31        50\n",
      "\n",
      "    accuracy                           0.43       500\n",
      "   macro avg       0.30      0.27      0.27       500\n",
      "weighted avg       0.41      0.43      0.41       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results on the MELD dataset using Logistic Regression:\\n\")\n",
    "print(classification_report(df_test[\"Emotion\"], lr_predictions, zero_division=0))\n",
    "model_scores[\"Logistic Regression\"] = f1_score(df_test[\"Emotion\"], lr_predictions, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93357b23-049a-4f46-b5ca-532cf2f938e2",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c60e5ee4-da62-49e6-81eb-5ca987db3efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC model is ready to use!\n"
     ]
    }
   ],
   "source": [
    "pipe_svc = Pipeline(steps=[('cv', CountVectorizer()), ('svc', SVC(kernel='rbf', C=10))])\n",
    "pipe_svc.fit(x_train, y_train)\n",
    "print(\"SVC model is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f7acf5-d2a5-4c51-85d5-5d9f6f6a29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_predictions = pipe_svc.predict(df_test[\"Utterance\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83fb89a-d312-4231-814b-b13e62e5a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on the MELD dataset using SVC:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.18      0.06      0.10        62\n",
      "     disgust       0.33      0.09      0.14        11\n",
      "        fear       0.00      0.00      0.00        10\n",
      "         joy       0.37      0.18      0.24        91\n",
      "     neutral       0.52      0.79      0.62       238\n",
      "     sadness       0.22      0.05      0.09        38\n",
      "    surprise       0.46      0.52      0.49        50\n",
      "\n",
      "    accuracy                           0.47       500\n",
      "   macro avg       0.30      0.24      0.24       500\n",
      "weighted avg       0.41      0.47      0.41       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results on the MELD dataset using SVC:\\n\")\n",
    "print(classification_report(df_test[\"Emotion\"], svc_predictions, zero_division=0))\n",
    "model_scores[\"SVC\"] = f1_score(df_test[\"Emotion\"], svc_predictions, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484433c-e461-4e9d-849e-93f8925dafb3",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ae52215-b874-46cc-afb5-c3eee2965920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier model is ready to use!\n"
     ]
    }
   ],
   "source": [
    "pipe_rf = Pipeline(steps=[('cv', CountVectorizer()), ('rf', RandomForestClassifier(n_estimators=10))])\n",
    "pipe_rf.fit(x_train, y_train)\n",
    "print(\"Random Forest Classifier model is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce39392-666f-47a5-aabc-7dd1e1d34737",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = pipe_rf.predict(df_test[\"Utterance\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b889b8ef-f618-4425-8060-ffd7e3b3ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on the MELD dataset using Random Forest Classifier:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.19      0.11      0.14        62\n",
      "     disgust       0.00      0.00      0.00        11\n",
      "        fear       0.00      0.00      0.00        10\n",
      "         joy       0.23      0.12      0.16        91\n",
      "     neutral       0.53      0.70      0.60       238\n",
      "     sadness       0.13      0.05      0.08        38\n",
      "    surprise       0.32      0.50      0.39        50\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.20      0.21      0.20       500\n",
      "weighted avg       0.36      0.42      0.38       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results on the MELD dataset using Random Forest Classifier:\\n\")\n",
    "print(classification_report(df_test[\"Emotion\"], rf_predictions, zero_division=0))\n",
    "model_scores[\"Random Forest Classifier\"] = f1_score(df_test[\"Emotion\"], rf_predictions, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "247afec1-f9eb-4418-a20c-27ad9c2c74ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1-score: 0.3785\n"
     ]
    }
   ],
   "source": [
    "weighted_f1 = f1_score(df_test[\"Emotion\"], rf_predictions, average='weighted', zero_division=0)\n",
    "print(f\"Weighted F1-score: {weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe11a1-f941-4f84-bf12-b90ead145cee",
   "metadata": {},
   "source": [
    "## 🔍 Zero-Shot Emotion Classification with `facebook/bart-large-mnli`\n",
    "To evaluate an alternative approach without supervised training, I use the [`facebook/bart-large-mnli`](https://huggingface.co/facebook/bart-large-mnli) model from Hugging Face for **zero-shot classification**. This model allows emotion classification by comparing each utterance directly against a set of candidate labels, without requiring task-specific fine-tuning.\n",
    "\n",
    "For each utterance, the model ranks the candidate emotions based on how well they match the text. The top-ranked label is selected as the predicted emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2219c9-b5a1-4837-b3ba-fd9386822576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "020ee098-43ee-471a-ada9-43b49ce97864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'surprise'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = zero_shot_classifier(example_text, candidate_labels=emotion_labels)\n",
    "result[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "822b2bc3-973a-4542-8e6a-5740e1ca91c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [01:19<00:00,  6.28it/s]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_predictions = []\n",
    "for text in tqdm(df_test[\"Utterance\"]):\n",
    "    output = zero_shot_classifier(text, candidate_labels=emotion_labels)\n",
    "    zero_shot_predictions.append(output[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9e5f4f-1f38-4884-b39a-d9bd59dbb0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on the MELD dataset using facebook/bart-large-mnli (zero-shot classification):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.55      0.10      0.16        62\n",
      "     disgust       0.21      0.27      0.24        11\n",
      "        fear       0.25      0.10      0.14        10\n",
      "         joy       0.82      0.35      0.49        91\n",
      "     neutral       0.70      0.14      0.23       238\n",
      "     sadness       0.75      0.16      0.26        38\n",
      "    surprise       0.13      0.96      0.22        50\n",
      "\n",
      "    accuracy                           0.26       500\n",
      "   macro avg       0.49      0.30      0.25       500\n",
      "weighted avg       0.63      0.26      0.27       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results on the MELD dataset using facebook/bart-large-mnli (zero-shot classification):\\n\")\n",
    "print(classification_report(df_test[\"Emotion\"], zero_shot_predictions, zero_division=0))\n",
    "model_scores[\"facebook/bart-large-mnli\"] = f1_score(df_test[\"Emotion\"], zero_shot_predictions, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62cdb17-ba97-4a1a-b0fe-104bf57008e7",
   "metadata": {},
   "source": [
    "## 🤖 Emotion Classification with `emotion-english-distilroberta-base`\n",
    "\n",
    "In this step, I evaluate a transformer-based model fine-tuned specifically for emotion classification: [`j-hartmann/emotion-english-distilroberta-base`](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base). This model is built on top of DistilRoBERTa and has been trained on multiple English emotion datasets.\n",
    "\n",
    "Using the Hugging Face `text-classification` pipeline, I apply the model to each utterance in the dataset and extract the top predicted emotion label. I then compare the predicted labels to the ground-truth annotations using standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53a1a5cd-b84a-4d50-a01e-d80c3872492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "distilroberta_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f0a059c-3160-412f-85dd-a652cbcdcc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'sadness', 'score': 0.8036125302314758}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = distilroberta_classifier(example_text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac2fbbeb-2648-4fd2-88f3-9ad31a5c28d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:06<00:00, 80.77it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_distilroberta = []\n",
    "for text in tqdm(df_test[\"Utterance\"]):\n",
    "    output = distilroberta_classifier(text, top_k=1)\n",
    "    predictions_distilroberta.append(output[0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c05e8211-030c-471c-879d-d263f020a5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on the MELD dataset using j-hartmann/emotion-english-distilroberta-base (text classification):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.29      0.32      0.31        62\n",
      "     disgust       0.06      0.27      0.10        11\n",
      "        fear       0.00      0.00      0.00        10\n",
      "         joy       0.59      0.43      0.50        91\n",
      "     neutral       0.75      0.55      0.64       238\n",
      "     sadness       0.50      0.29      0.37        38\n",
      "    surprise       0.33      0.74      0.46        50\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.36      0.37      0.34       500\n",
      "weighted avg       0.57      0.48      0.51       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation results on the MELD dataset using j-hartmann/emotion-english-distilroberta-base (text classification):\\n\")\n",
    "print(classification_report(df_test[\"Emotion\"], predictions_distilroberta, zero_division=0))\n",
    "model_scores[\"j-hartmann/emotion-english-distilroberta-base\"] = f1_score(df_test[\"Emotion\"], predictions_distilroberta, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8f041-7815-43d1-b52b-a3b37cb8f675",
   "metadata": {},
   "source": [
    "## 🤖 Emotion Classification using Large Language Models and Prompt Engineering\n",
    "\n",
    "In this section, I evaluate the emotion classification capabilities of several large language models (LLMs) using **prompt-based inference**, without task-specific fine-tuning. This approach leverages the few-shot and instruction-following capabilities of modern LLMs for **in-context learning**.\n",
    "\n",
    "#### 💡 Models Used\n",
    "\n",
    "I tested the following LLMs across various prompting strategies:\n",
    "\n",
    "* **`llama3.1:8b`** via **Ollama**\n",
    "* **`gpt-4.1-nano`** via **OpenAI API**\n",
    "* **`gemini-2.5-flash-lite`** via **Vertex AI**\n",
    "\n",
    "These models were evaluated using a consistent framework to ensure comparability.\n",
    "\n",
    "#### 🧠 Prompting Strategies\n",
    "\n",
    "To understand how different types of prompts affect performance, I evaluated each model with the following five prompt types:\n",
    "\n",
    "1. **Zero-shot prompt**: The model receives only the target utterance and is expected to infer the emotion label from a predefined list, with no prior examples.\n",
    "2. **One-shot prompt**: A single labeled example is provided before the target utterance.\n",
    "3. **Few-shot prompt**: Multiple labeled examples (covering all emotion classes) are provided to guide the model's prediction.\n",
    "4. **Contextual prompt**: The model receives the target utterance along with the preceding utterances from the same conversation to account for dialogue context.\n",
    "5. **Retrieval-Augmented Contextual prompt**: In addition to the conversation history, the model is shown one **relevant example** from the training dataset, retrieved via semantic similarity using a **vector store (FAISS)**. This combines **retrieval-augmented generation (RAG)** principles with conversational context.\n",
    "\n",
    "#### 🧰 Framework Components\n",
    "\n",
    "To support this evaluation, I implemented several utility classes:\n",
    "\n",
    "* **`MELDDataManager`**: Organizes utterances by dialogue and retrieves conversation history given a dialogue and utterance ID.\n",
    "* **`DemonstrationRetriever`**: Builds a semantic vector store using sentence embeddings (`all-MiniLM-L6-v2`) and retrieves the most similar utterance from the training set for use in retrieval-augmented prompts.\n",
    "* **`PromptGenerator`**: Generates different prompt templates in a modular way, including support for history and demonstrations.\n",
    "* **`evaluate_emotion_classification`**: A unified evaluation function that constructs the prompt, queries the model, extracts predicted labels, and reports performance metrics.\n",
    "\n",
    "#### 📊 Evaluation\n",
    "\n",
    "Each model was tested on the same MELD test split with all five prompt types. The outputs were evaluated using **precision, recall, and F1-score**, and the predicted labels were extracted using a rule-based parser that identifies the first valid emotion label in the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec14725f-95cf-46fc-87b6-32089ba3377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDDataManager:\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.df = dataframe.copy()\n",
    "        self.dialogues: Dict[int, List[Dict[str, Any]]] = {}\n",
    "        for dialogue_id, group in self.df.groupby('Dialogue_ID'):\n",
    "            self.dialogues[dialogue_id] = group.sort_values(by='Utterance_ID').to_dict('records')\n",
    "\n",
    "    def get_conversation_history(self, dialogue_id: int, utterance_id: int, window_size: int = 9) -> List[Dict[str, Any]]:\n",
    "        if dialogue_id not in self.dialogues: return []\n",
    "        conversation = self.dialogues[dialogue_id]\n",
    "        target_idx = next((i for i, u in enumerate(conversation) if u['Utterance_ID'] == utterance_id), -1)\n",
    "        if target_idx == -1: return []\n",
    "        start_idx = max(0, target_idx - window_size)\n",
    "        return conversation[start_idx:target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c6e4759-781b-4785-a28c-fb415962cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemonstrationRetriever:\n",
    "    \"\"\"\n",
    "    Handles creating an in-memory vector store and retrieving demonstrations.\n",
    "    This version does NOT save the vector store to disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_df: pd.DataFrame, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initializes the retriever by creating an in-memory FAISS vector store from train_df.\n",
    "        \"\"\"\n",
    "        print(\"Initializing DemonstrationRetriever and creating in-memory vector store...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        documents = train_df['Utterance'].tolist()\n",
    "        metadatas = train_df[['Emotion']].to_dict('records')\n",
    "        self.vector_store = FAISS.from_texts(documents, self.embeddings, metadatas=metadatas)\n",
    "        print(\"In-memory vector store created successfully.\")\n",
    "\n",
    "    def retrieve_demonstration(self, query_utterance: str) -> str:\n",
    "        results = self.vector_store.similarity_search(query_utterance, k=1)\n",
    "        if not results: return \"No demonstration found.\"\n",
    "        retrieved_doc = results[0]\n",
    "        return f\"Utterance: \\\"{retrieved_doc.page_content}\\\"\\nEmotion: {retrieved_doc.metadata.get('Emotion', 'N/A')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe78245e-d44d-46ee-9bc4-812bfa847201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptGenerator:\n",
    "    \"\"\"\n",
    "    A toolkit for generating various ChatPromptTemplates for emotion recognition.\n",
    "    \"\"\"\n",
    "    MELD_EMOTION_LABELS = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "\n",
    "    def _format_history(self, history: List[Dict[str, Any]]) -> str:\n",
    "        if not history: return \"This is the first utterance in the conversation.\"\n",
    "        return \"\\n\".join([f\"{u['Speaker']}: \\\"{u['Utterance']}\\\"\" for u in history])\n",
    "\n",
    "    def _format_target_utterance(self, target_utterance: Dict[str, Any]) -> str:\n",
    "        return f\"{target_utterance['Speaker']}: \\\"{target_utterance['Utterance']}\\\"\"\n",
    "\n",
    "    def get_zero_shot_template(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Returns a basic zero-shot classification prompt.\"\"\"\n",
    "        emotion_options = \", \".join(self.MELD_EMOTION_LABELS)\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"You are a classification assistant. Only output one of the following emotion labels: {emotion_options}. No explanation.\"),\n",
    "            (\"user\", \"Utterance: {utterance}\\nEmotion:\")\n",
    "        ])\n",
    "        template.name = \"zero_shot_prompt\"\n",
    "        return template\n",
    "\n",
    "    def get_one_shot_template(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Returns a one-shot classification prompt with a single example.\"\"\"\n",
    "        emotion_options = \", \".join(self.MELD_EMOTION_LABELS)\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"You are an assistant that labels user utterances with one of the following emotions: {emotion_options}. Please give only the label!\"),\n",
    "            (\"user\", \"Utterance: That’s fine. I’m okay with it. → Emotion: neutral\\nUtterance: {utterance} → Emotion:\")\n",
    "        ])\n",
    "        template.name = \"one_shot_prompt\"\n",
    "        return template\n",
    "\n",
    "    def get_few_shot_template(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Returns a few-shot classification prompt with multiple examples.\"\"\"\n",
    "        emotion_options = \", \".join(self.MELD_EMOTION_LABELS)\n",
    "        template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"You are an assistant that labels user utterances with one of the following emotions: {emotion_options}. Please give only the label!\"),\n",
    "            (\"user\", \"\"\"Examples:\n",
    "\n",
    "Utterance: I can’t believe this happened to me.\n",
    "Emotion: sadness\n",
    "\n",
    "Utterance: Wow, that’s amazing!\n",
    "Emotion: joy\n",
    "\n",
    "Utterance: Why would you do that?!\n",
    "Emotion: anger\n",
    "\n",
    "Utterance: That’s fine. I’m okay with it.\n",
    "Emotion: neutral\n",
    "\n",
    "Utterance: Oh my God, I didn’t expect that!\n",
    "Emotion: surprise\n",
    "\n",
    "Utterance: Yuck, that’s gross.\n",
    "Emotion: disgust\n",
    "\n",
    "Utterance: I’m really scared.\n",
    "Emotion: fear\n",
    "\n",
    "Now classify:\n",
    "Utterance: \"{utterance}\"\n",
    "Emotion:\"\"\")\n",
    "        ])\n",
    "        template.name = \"few_shot_prompt\"\n",
    "        return template\n",
    "\n",
    "    def get_contextual_template(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Returns an InstructERC-style prompt with history but no demonstration.\"\"\"\n",
    "        emotion_options = \", \".join(self.MELD_EMOTION_LABELS)\n",
    "        system_template = (\"You are an expert in conversational analysis. Your task is to identify the emotion of the \"\n",
    "                           \"TARGET UTTERANCE based on the CONVERSATION HISTORY. \"\n",
    "                           f\"You must only output one of the following emotion labels: <{emotion_options}>. Do not provide any explanation.\")\n",
    "        user_template = (\"### CONVERSATION HISTORY ###\\n{history}\\n\\n\"\n",
    "                         \"### TARGET UTTERANCE ###\\n{target_utterance_with_speaker}\\n\\n\"\n",
    "                         \"Emotion:\")\n",
    "        template = ChatPromptTemplate.from_messages([(\"system\", system_template), (\"user\", user_template)])\n",
    "        template.name = \"contextual_prompt\"\n",
    "        return template\n",
    "\n",
    "    def get_retrieval_augmented_contextual_template(self) -> ChatPromptTemplate:\n",
    "        \"\"\"Returns the full InstructERC-style prompt with history and a demonstration.\"\"\"\n",
    "        emotion_options = \", \".join(self.MELD_EMOTION_LABELS)\n",
    "        system_template = (\"You are an expert in conversational analysis. Your task is to identify the emotion of the \"\n",
    "                           \"TARGET UTTERANCE based on the provided DEMONSTRATION example and the CONVERSATION HISTORY. \"\n",
    "                           f\"You must only output one of the following emotion labels: <{emotion_options}>. Do not provide any explanation.\")\n",
    "        user_template = (\"### DEMONSTRATION ###\\n{demonstration}\\n\\n\"\n",
    "                         \"### CONVERSATION HISTORY ###\\n{history}\\n\\n\"\n",
    "                         \"### TARGET UTTERANCE ###\\n{target_utterance_with_speaker}\\n\\n\"\n",
    "                         \"Emotion:\")\n",
    "        template = ChatPromptTemplate.from_messages([(\"system\", system_template), (\"user\", user_template)])\n",
    "        template.name = \"retrieval_augmented_contextual_prompt\"\n",
    "        return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "570b0827-3a08-4c01-95ec-4b7142fb8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion_from_output(output_text: str, valid_labels: List[str]) -> Optional[str]:\n",
    "    output_text = output_text.lower()\n",
    "    first_found = None\n",
    "    first_index = len(output_text) + 1\n",
    "    for label in valid_labels:\n",
    "        try:\n",
    "            idx = output_text.find(label)\n",
    "            if idx != -1 and idx < first_index:\n",
    "                first_index = idx\n",
    "                first_found = label\n",
    "        except AttributeError:\n",
    "            return \"neutral\"\n",
    "    return first_found if first_found else \"neutral\"\n",
    "\n",
    "def print_classification_report(labels: List[str], predictions: List[str], llm_name: str, prompt_name: str):\n",
    "    print(f\"\\n📌 Evaluating with model: {llm_name}\")\n",
    "    print(f\"📄 Prompt Template Type: {prompt_name}\\n\")\n",
    "    print(classification_report(labels, predictions, zero_division=0))\n",
    "    \n",
    "def evaluate_emotion_classification(df: pd.DataFrame, llm, prompt: ChatPromptTemplate, data_manager: MELDDataManager, retriever: Optional[DemonstrationRetriever] = None, limit: Optional[int] = None, verbose: bool = False, print_report: bool = False):\n",
    "    if verbose:\n",
    "        print(f\"\\n📌 Evaluating with model: {llm.model}\")\n",
    "        print(f\"📄 Prompt Template Type: {prompt.name}\")\n",
    "    results = []\n",
    "    df_subset = df.head(limit) if limit else df\n",
    "    for idx, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=f\"Evaluating {prompt.name} using {llm.name}\"):\n",
    "        input_dict = {}\n",
    "        if prompt.name in [\"zero_shot_prompt\", \"one_shot_prompt\", \"few_shot_prompt\"]:\n",
    "            input_dict = {\"utterance\": row['Utterance']}\n",
    "        elif prompt.name in [\"contextual_prompt\", \"retrieval_augmented_contextual_prompt\"]:\n",
    "            history_list = data_manager.get_conversation_history(row['Dialogue_ID'], row['Utterance_ID'])\n",
    "            input_dict[\"history\"] = prompt_generator._format_history(history_list)\n",
    "            input_dict[\"target_utterance_with_speaker\"] = prompt_generator._format_target_utterance(row)\n",
    "            if prompt.name == \"retrieval_augmented_contextual_prompt\":\n",
    "                input_dict[\"demonstration\"] = retriever.retrieve_demonstration(row['Utterance'])\n",
    "        prompt_input = prompt.invoke(input_dict)\n",
    "        raw_output = llm.invoke(prompt_input)\n",
    "        if isinstance(raw_output, AIMessage):\n",
    "            raw_output = raw_output.content\n",
    "        predicted_emotion = extract_emotion_from_output(raw_output, PromptGenerator.MELD_EMOTION_LABELS)\n",
    "        results.append(predicted_emotion)\n",
    "    \n",
    "    model_scores[f\"{llm.name} with {prompt.name}\"] = f1_score(df_test[\"Emotion\"], results, average='weighted', zero_division=0)\n",
    "    \n",
    "    if print_report:\n",
    "        print_classification_report(df_subset[\"Emotion\"].tolist(), results, llm.name, prompt.name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "477778dc-f689-411a-95df-3b9f2844552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DemonstrationRetriever and creating in-memory vector store...\n",
      "In-memory vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "retriever = DemonstrationRetriever(train_df=df_train)\n",
    "data_manager = MELDDataManager(df_test)\n",
    "prompt_generator = PromptGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f4105-d4cc-4275-b4b0-6a159e8101e3",
   "metadata": {},
   "source": [
    "### LLM: \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85f4ca3e-7809-49cf-88c9-c34b6dce5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3_8b = OllamaLLM(model=\"llama3.1:8b\")\n",
    "llama3_8b.name = \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0f607d2-2a35-4b30-95b4-01e2f6ef7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating zero_shot_prompt using llama3.1:8b: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [02:15<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: llama3.1:8b\n",
      "📄 Prompt Template Type: zero_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.35      0.37      0.36        62\n",
      "     disgust       0.12      0.36      0.19        11\n",
      "        fear       0.09      0.10      0.10        10\n",
      "         joy       0.55      0.55      0.55        91\n",
      "     neutral       0.72      0.63      0.67       238\n",
      "     sadness       0.47      0.53      0.49        38\n",
      "    surprise       0.37      0.36      0.36        50\n",
      "\n",
      "    accuracy                           0.53       500\n",
      "   macro avg       0.38      0.41      0.39       500\n",
      "weighted avg       0.56      0.53      0.54       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=llama3_8b, prompt=prompt_generator.get_zero_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "097427c2-3df2-4124-96ef-941c22c2b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating one_shot_prompt using llama3.1:8b: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [02:08<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: llama3.1:8b\n",
      "📄 Prompt Template Type: one_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.32      0.44      0.37        62\n",
      "     disgust       0.06      0.27      0.10        11\n",
      "        fear       0.11      0.20      0.14        10\n",
      "         joy       0.61      0.51      0.55        91\n",
      "     neutral       0.84      0.38      0.53       238\n",
      "     sadness       0.44      0.50      0.47        38\n",
      "    surprise       0.29      0.68      0.40        50\n",
      "\n",
      "    accuracy                           0.44       500\n",
      "   macro avg       0.38      0.43      0.36       500\n",
      "weighted avg       0.62      0.44      0.48       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=llama3_8b, prompt=prompt_generator.get_one_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f09b0e7-9a98-45d4-9fbd-3168b5d752e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating few_shot_prompt using llama3.1:8b: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [01:57<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: llama3.1:8b\n",
      "📄 Prompt Template Type: few_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.36      0.37      0.37        62\n",
      "     disgust       0.10      0.27      0.15        11\n",
      "        fear       0.06      0.10      0.08        10\n",
      "         joy       0.70      0.47      0.57        91\n",
      "     neutral       0.77      0.61      0.68       238\n",
      "     sadness       0.33      0.63      0.44        38\n",
      "    surprise       0.39      0.56      0.46        50\n",
      "\n",
      "    accuracy                           0.53       500\n",
      "   macro avg       0.39      0.43      0.39       500\n",
      "weighted avg       0.61      0.53      0.56       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=llama3_8b, prompt=prompt_generator.get_few_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d92688ac-a8c8-474e-9d50-f34c74eb58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating contextual_prompt using llama3.1:8b: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:39<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: llama3.1:8b\n",
      "📄 Prompt Template Type: contextual_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.37      0.58      0.45        62\n",
      "     disgust       0.25      0.64      0.36        11\n",
      "        fear       0.17      0.10      0.12        10\n",
      "         joy       0.50      0.43      0.46        91\n",
      "     neutral       0.73      0.38      0.50       238\n",
      "     sadness       0.30      0.63      0.41        38\n",
      "    surprise       0.29      0.52      0.37        50\n",
      "\n",
      "    accuracy                           0.45       500\n",
      "   macro avg       0.37      0.47      0.38       500\n",
      "weighted avg       0.55      0.45      0.46       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=llama3_8b, prompt=prompt_generator.get_contextual_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf839cc6-4be7-4454-939b-70393d6aaaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval_augmented_contextual_prompt using llama3.1:8b: 100%|██████████████████████████████████████████████████████████████████████████| 500/500 [05:35<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: llama3.1:8b\n",
      "📄 Prompt Template Type: retrieval_augmented_contextual_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.29      0.65      0.40        62\n",
      "     disgust       0.11      0.18      0.14        11\n",
      "        fear       0.20      0.10      0.13        10\n",
      "         joy       0.62      0.45      0.52        91\n",
      "     neutral       0.82      0.41      0.54       238\n",
      "     sadness       0.23      0.45      0.31        38\n",
      "    surprise       0.29      0.48      0.36        50\n",
      "\n",
      "    accuracy                           0.44       500\n",
      "   macro avg       0.37      0.39      0.34       500\n",
      "weighted avg       0.59      0.44      0.47       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=llama3_8b, prompt=prompt_generator.get_retrieval_augmented_contextual_template(),\n",
    "    data_manager=data_manager, retriever=retriever, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72aa051-e7e1-4713-85d8-55f2c207df39",
   "metadata": {},
   "source": [
    "### LLM: \"gpt-4.1-nano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbd8a5fe-7913-4326-a50f-c40d5a8c35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_1_nano = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
    "gpt_4_1_nano.name = \"gpt_4_1_nano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49b43e96-23e3-4595-9a0a-bc82f6cf3011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating zero_shot_prompt using gpt_4_1_nano: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:53<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gpt_4_1_nano\n",
      "📄 Prompt Template Type: zero_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.40      0.29      0.34        62\n",
      "     disgust       0.10      0.18      0.12        11\n",
      "        fear       0.09      0.10      0.10        10\n",
      "         joy       0.42      0.70      0.52        91\n",
      "     neutral       0.81      0.58      0.67       238\n",
      "     sadness       0.70      0.37      0.48        38\n",
      "    surprise       0.42      0.68      0.52        50\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.42      0.41      0.39       500\n",
      "weighted avg       0.61      0.54      0.55       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gpt_4_1_nano, prompt=prompt_generator.get_zero_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ce41e65-0249-4390-9b22-6066a3dc14a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating one_shot_prompt using gpt_4_1_nano: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:38<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gpt_4_1_nano\n",
      "📄 Prompt Template Type: one_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.39      0.50      0.44        62\n",
      "     disgust       0.09      0.18      0.12        11\n",
      "        fear       0.25      0.10      0.14        10\n",
      "         joy       0.42      0.60      0.49        91\n",
      "     neutral       0.92      0.43      0.58       238\n",
      "     sadness       0.56      0.37      0.44        38\n",
      "    surprise       0.32      0.82      0.46        50\n",
      "\n",
      "    accuracy                           0.49       500\n",
      "   macro avg       0.42      0.43      0.38       500\n",
      "weighted avg       0.64      0.49      0.51       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gpt_4_1_nano, prompt=prompt_generator.get_one_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42bfa99d-f9d5-4ce9-9942-b50365f1f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating few_shot_prompt using gpt_4_1_nano: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:31<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gpt_4_1_nano\n",
      "📄 Prompt Template Type: few_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.45      0.45      0.45        62\n",
      "     disgust       0.15      0.18      0.17        11\n",
      "        fear       0.20      0.30      0.24        10\n",
      "         joy       0.57      0.67      0.62        91\n",
      "     neutral       0.80      0.72      0.76       238\n",
      "     sadness       0.69      0.47      0.56        38\n",
      "    surprise       0.48      0.58      0.52        50\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.48      0.48      0.47       500\n",
      "weighted avg       0.65      0.63      0.63       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gpt_4_1_nano, prompt=prompt_generator.get_few_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d518d9c-a45e-434f-a0f8-9ce58f0efd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating contextual_prompt using gpt_4_1_nano: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:42<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gpt_4_1_nano\n",
      "📄 Prompt Template Type: contextual_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.68      0.21      0.32        62\n",
      "     disgust       0.20      0.55      0.29        11\n",
      "        fear       0.43      0.30      0.35        10\n",
      "         joy       0.40      0.64      0.49        91\n",
      "     neutral       0.70      0.65      0.67       238\n",
      "     sadness       0.76      0.34      0.47        38\n",
      "    surprise       0.43      0.52      0.47        50\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.51      0.46      0.44       500\n",
      "weighted avg       0.60      0.55      0.54       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gpt_4_1_nano, prompt=prompt_generator.get_contextual_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f2ea609-519d-4c7a-a32e-fdc232786740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval_augmented_contextual_prompt using gpt_4_1_nano: 100%|█████████████████████████████████████████████████████████████████████████| 500/500 [04:02<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gpt_4_1_nano\n",
      "📄 Prompt Template Type: retrieval_augmented_contextual_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.19      0.30        62\n",
      "     disgust       0.11      0.55      0.18        11\n",
      "        fear       0.50      0.30      0.38        10\n",
      "         joy       0.46      0.64      0.53        91\n",
      "     neutral       0.78      0.62      0.69       238\n",
      "     sadness       0.74      0.37      0.49        38\n",
      "    surprise       0.34      0.58      0.43        50\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.51      0.46      0.43       500\n",
      "weighted avg       0.64      0.54      0.56       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gpt_4_1_nano, prompt=prompt_generator.get_retrieval_augmented_contextual_template(),\n",
    "    data_manager=data_manager, retriever=retriever, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245d9ba-eb85-41b9-8d5e-c88348374a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc13715-786d-4c9e-8b05-f851c945e7e7",
   "metadata": {},
   "source": [
    "### LLM: \"gemini-2.5-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ab79a53-8fa7-4424-a0da-324901e9e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_2_5_flash_lite = ChatVertexAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "gemini_2_5_flash_lite.name = \"gemini-2.5-flash-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a25d2590-4d6a-4026-9f96-104b99832a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating zero_shot_prompt using gemini-2.5-flash-lite: 100%|█████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:04<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gemini-2.5-flash-lite\n",
      "📄 Prompt Template Type: zero_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.56      0.29      0.38        62\n",
      "     disgust       0.16      0.45      0.23        11\n",
      "        fear       0.22      0.20      0.21        10\n",
      "         joy       0.54      0.62      0.58        91\n",
      "     neutral       0.75      0.72      0.73       238\n",
      "     sadness       0.46      0.55      0.50        38\n",
      "    surprise       0.50      0.50      0.50        50\n",
      "\n",
      "    accuracy                           0.60       500\n",
      "   macro avg       0.46      0.48      0.45       500\n",
      "weighted avg       0.62      0.60      0.60       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gemini_2_5_flash_lite, prompt=prompt_generator.get_zero_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "125173ed-3f15-414f-ab5f-81ce2ce77170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating one_shot_prompt using gemini-2.5-flash-lite: 100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:26<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gemini-2.5-flash-lite\n",
      "📄 Prompt Template Type: one_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.45      0.40      0.42        62\n",
      "     disgust       0.15      0.55      0.24        11\n",
      "        fear       0.15      0.20      0.17        10\n",
      "         joy       0.53      0.57      0.55        91\n",
      "     neutral       0.83      0.68      0.74       238\n",
      "     sadness       0.50      0.50      0.50        38\n",
      "    surprise       0.52      0.62      0.56        50\n",
      "\n",
      "    accuracy                           0.59       500\n",
      "   macro avg       0.45      0.50      0.46       500\n",
      "weighted avg       0.64      0.59      0.61       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gemini_2_5_flash_lite, prompt=prompt_generator.get_one_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fb47f5c-7e7e-406d-9071-2e32972b70b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating few_shot_prompt using gemini-2.5-flash-lite: 100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:11<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gemini-2.5-flash-lite\n",
      "📄 Prompt Template Type: few_shot_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.39      0.43        62\n",
      "     disgust       0.31      0.36      0.33        11\n",
      "        fear       0.17      0.10      0.12        10\n",
      "         joy       0.73      0.49      0.59        91\n",
      "     neutral       0.70      0.87      0.78       238\n",
      "     sadness       0.53      0.50      0.51        38\n",
      "    surprise       0.62      0.46      0.53        50\n",
      "\n",
      "    accuracy                           0.65       500\n",
      "   macro avg       0.51      0.45      0.47       500\n",
      "weighted avg       0.64      0.65      0.63       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gemini_2_5_flash_lite, prompt=prompt_generator.get_few_shot_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8e38424-79c5-4e3b-9ce1-c187664c0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating contextual_prompt using gemini-2.5-flash-lite: 100%|████████████████████████████████████████████████████████████████████████████████████| 500/500 [03:06<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gemini-2.5-flash-lite\n",
      "📄 Prompt Template Type: contextual_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.27      0.37        62\n",
      "     disgust       0.18      0.27      0.21        11\n",
      "        fear       0.50      0.20      0.29        10\n",
      "         joy       0.69      0.58      0.63        91\n",
      "     neutral       0.72      0.88      0.79       238\n",
      "     sadness       0.47      0.58      0.52        38\n",
      "    surprise       0.65      0.44      0.52        50\n",
      "\n",
      "    accuracy                           0.66       500\n",
      "   macro avg       0.54      0.46      0.48       500\n",
      "weighted avg       0.65      0.66      0.64       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gemini_2_5_flash_lite, prompt=prompt_generator.get_contextual_template(),\n",
    "    data_manager=data_manager, print_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c541f4c7-be17-4a02-b165-2a71316a827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval_augmented_contextual_prompt using gemini-2.5-flash-lite: 100%|████████████████████████████████████████████████████████████████| 500/500 [03:58<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Evaluating with model: gemini-2.5-flash-lite\n",
      "📄 Prompt Template Type: retrieval_augmented_contextual_prompt\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.19      0.28        62\n",
      "     disgust       0.23      0.45      0.30        11\n",
      "        fear       0.18      0.20      0.19        10\n",
      "         joy       0.67      0.56      0.61        91\n",
      "     neutral       0.72      0.83      0.77       238\n",
      "     sadness       0.46      0.55      0.50        38\n",
      "    surprise       0.46      0.44      0.45        50\n",
      "\n",
      "    accuracy                           0.62       500\n",
      "   macro avg       0.46      0.46      0.44       500\n",
      "weighted avg       0.62      0.62      0.61       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_emotion_classification(\n",
    "    df=df_test, llm=gemini_2_5_flash_lite, prompt=prompt_generator.get_retrieval_augmented_contextual_template(),\n",
    "    data_manager=data_manager, retriever=retriever, print_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d8682-8824-4f13-a86c-e4882f931dd9",
   "metadata": {},
   "source": [
    "### 📊 Visualizing Model Performance with Weighted F1 Scores\n",
    "\n",
    "To compare the effectiveness of different models on the MELD dataset, I visualize their **weighted F1 scores** using a horizontal bar chart. The weighted F1 score accounts for class imbalance by weighting each class’s F1 score by its support (i.e., number of true instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6f2c6b5-427e-4012-b263-025555a14d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "orientation": "h",
         "text": [
          "0.41",
          "0.41",
          "0.38",
          "0.27",
          "0.51",
          "0.54",
          "0.48",
          "0.56",
          "0.46",
          "0.47",
          "0.55",
          "0.51",
          "0.63",
          "0.54",
          "0.56",
          "0.60",
          "0.61",
          "0.63",
          "0.64",
          "0.61"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          0.4142199023862466,
          0.4106464230661797,
          0.37847036707854986,
          0.27066080600205833,
          0.5071620147444986,
          0.5446341579190526,
          0.47749235989143385,
          0.555468017443709,
          0.4564786586372022,
          0.4689173028067897,
          0.5508999384801189,
          0.5081648353858624,
          0.6322799365845622,
          0.5449878932392369,
          0.555734871239899,
          0.5992273878301296,
          0.6096857941627193,
          0.6325377332157658,
          0.6387250246978328,
          0.6077038410124529
         ],
         "y": [
          "Logistic Regression",
          "SVC",
          "Random Forest Classifier",
          "facebook/bart-large-mnli",
          "j-hartmann/emotion-english-distilroberta-base",
          "llama3.1:8b with zero_shot_prompt",
          "llama3.1:8b with one_shot_prompt",
          "llama3.1:8b with few_shot_prompt",
          "llama3.1:8b with contextual_prompt",
          "llama3.1:8b with retrieval_augmented_contextual_prompt",
          "gpt_4_1_nano with zero_shot_prompt",
          "gpt_4_1_nano with one_shot_prompt",
          "gpt_4_1_nano with few_shot_prompt",
          "gpt_4_1_nano with contextual_prompt",
          "gpt_4_1_nano with retrieval_augmented_contextual_prompt",
          "gemini-2.5-flash-lite with zero_shot_prompt",
          "gemini-2.5-flash-lite with one_shot_prompt",
          "gemini-2.5-flash-lite with few_shot_prompt",
          "gemini-2.5-flash-lite with contextual_prompt",
          "gemini-2.5-flash-lite with retrieval_augmented_contextual_prompt"
         ]
        }
       ],
       "layout": {
        "height": 600,
        "margin": {
         "b": 50,
         "l": 50,
         "r": 50,
         "t": 50
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Comparison by Weighted F1 Score on the MELD dataset"
        },
        "width": 1200,
        "xaxis": {
         "range": [
          0,
          1
         ],
         "title": {
          "text": "Weighted F1 Score"
         }
        },
        "yaxis": {
         "title": {
          "text": "Model"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure(go.Bar(\n",
    "    x=list(model_scores.values()),\n",
    "    y=list(model_scores.keys()),\n",
    "    orientation='h',\n",
    "    text=[f\"{v:.2f}\" for v in model_scores.values()],\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Model Comparison by Weighted F1 Score on the MELD dataset\",\n",
    "    xaxis_title=\"Weighted F1 Score\",\n",
    "    yaxis_title=\"Model\",\n",
    "    xaxis_range=[0, 1],\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c298584-0c20-4688-8ec5-90eebc1f52ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion Recognition",
   "language": "python",
   "name": "emo-rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
